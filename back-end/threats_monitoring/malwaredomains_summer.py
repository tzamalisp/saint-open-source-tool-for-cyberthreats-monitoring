from downloader import Downloader
from descriptive_analysis import DescriptiveAnalysis
from export_collection_data import ExportCollectionData
from pymongo import MongoClient
from pymongo.errors import PyMongoError
from bs4 import BeautifulSoup
from datetime import datetime
import ssl
import re

ssl._create_default_https_context = ssl._create_unverified_context


def connect_to_mongodb():
    """ This function implements the connection to mongoDB
        @returns
            connection  (MongoClient or None)   a MongoClient object to handle the connection. None on failure
    """

    # connect to database
    try:
        # connect to database
        connection = MongoClient('XXX.XXX.XXX.XXX', 27017)
        db = connection.admin
        db.authenticate('xxxxxx', 'xxxXXXxxxXX')
        return db

    except PyMongoError as e:
        print("Connection to Data Base failed: ", e)
        return None

# -------------------------------------------------------------------------------------------------------------------- #


def crawl_malware_domains(url):
    """ This function crawls the malware domain indicator and returns all the dataset links to be downloaded and scraped
        later.
    @param
        url     (string)        url of the indicator web page
    @return
    """

    print('Crawling site: ', url)
    downloader = Downloader()
    print(url)
    html = downloader(url)

    soup = BeautifulSoup(html, 'html5lib')
    possible_links = soup.find_all('a')

    htmlLinks, htmlRemovedLinks = list([]), list([])

    for link in possible_links :
        if link.has_attr('href') and link.attrs['href'][0].isdigit():
            # construct full path using function parameter url = 'https://mirror.uce.edu.ec/malwaredomains/'
            full_link = '{}{}'.format(url, link.attrs['href'])
            htmlLinks.append(full_link)
        elif link.has_attr('href') and link.attrs['href'].startswith('removed-domains-'):
            # in this loop we gather all the removed ip lists
            full_link = '{}{}'.format(url, link.attrs['href'])
            htmlRemovedLinks.append(full_link)

    return {"blocked": htmlLinks, "removed":  htmlRemovedLinks}

# -------------------------------------------------------------------------------------------------------------------- #


def scrape_and_model_content(html_links):

    modeled_data = list([])
    downloader = Downloader()
    exp = re.compile("""[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]""")

    for link in html_links["blocked"]:
        content = downloader(link)

        for row in content.split('\n'):

            data = [word for word in row.strip('\t').split('\t')]

            if len(data) != 4:
                continue

            date_regex = exp.match(data[3])

            if date_regex is not None:
                # prepare time values from indicators
                data[3] = date_regex.group()
                datetimeObject = datetime.strptime(data[3], '%Y%m%d')
                datetime_utc_string = str(datetimeObject.strftime('%Y-%m-%d'))
                datetime_utc = datetime.strptime(datetime_utc_string,'%Y-%m-%d')
                timestamp_utc = float(datetime_utc.timestamp())
            else:
                continue

            # prepare CTI time
            datetime_utc_cti_string = str(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'))
            datetime_utc_cti = datetime.strptime(datetime_utc_cti_string, '%Y-%m-%d %H:%M:%S')
            timestamp_utc_cti = float(datetime_utc_cti.timestamp())

            data_dict = {
                "Category": "Malware",
                "Entity-Type": "Domain",
                "Domain": data[0],
                "Subcategory": data[1],
                "Submitted-By": data[2],
                "TimestampUTC": timestamp_utc,
                "mongoDate": datetime_utc,
                "DatetimeUTC": datetime_utc_string,
                "TimestampUTC-CTI": timestamp_utc_cti,
                "mongoDate-CTI": datetime_utc_cti,
                "DatetimeUTC-CTI": datetime_utc_cti_string,
                "State": "Blocked"
            }

            modeled_data.append(data_dict)

    for link in html_links["removed"]:
           content = downloader(link)

           for row in content.split('\n'):
                data = [word for word in row.strip('\r').split('\t') if word != '#' and word != '']

                if len(data) != 4:
                    continue

                date_regex = exp.match(data[3])
                if date_regex is not None:
                    # prepare time values from indicators
                    data[3] = date_regex.group()
                    datetimeObject = datetime.strptime(data[3], '%Y%m%d')
                    datetime_utc_string = str(datetimeObject.strftime('%Y-%m-%d'))
                    datetime_utc = datetime.strptime(datetime_utc_string,'%Y-%m-%d')
                    timestamp_utc = float(datetime_utc.timestamp())
                else:
                    continue

                # prepare CTI time
                datetime_utc_cti_string = str(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'))
                datetime_utc_cti = datetime.strptime(datetime_utc_cti_string, '%Y-%m-%d %H:%M:%S')
                timestamp_utc_cti = float(datetime_utc_cti.timestamp())

                data_dict = {
                    "Category": "Malware",
                    "Entity-Type": "Domain",
                    "Domain": data[0],
                    "Subcategory": data[1],
                    "Submitted-By": data[2],
                    "TimestampUTC": timestamp_utc,
                    "mongoDate": datetime_utc,
                    "DatetimeUTC": datetime_utc_string,
                    "TimestampUTC-CTI": timestamp_utc_cti,
                    "mongoDate-CTI": datetime_utc_cti,
                    "DatetimeUTC-CTI": datetime_utc_cti_string,
                    "State": "Removed"
                }

                modeled_data.append(data_dict)

    return  modeled_data

# -------------------------------------------------------------------------------------------------------------------- #


if __name__ == "__main__":

    print("Report on", datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'), '\n')

    ''' Scrape Indicator '''
    # URL of the indicator
    URL = 'https://mirror.uce.edu.ec/malwaredomains/'
    # crawl indicator
    html_links = crawl_malware_domains(URL)
    # scrape data from links
    data = scrape_and_model_content(html_links)

    ''' Store instance of scraped data in MongoDB '''
    db = connect_to_mongodb()
    db.threats.malwaredomains.drop()
    res = db.threats.malwaredomains.insert_many(data)
    print("Documents Inserted: ", len(res.inserted_ids))

    """ Set Path Of Data Exportation """
    # to run on server
    server_path = '/var/www/html/saint/indicators2018/malware/'
    # to run locally
    local_path = ""

    """ Descriptive Analysis and Result Exportation"""
    analysis = DescriptiveAnalysis(collection=db.threats.malwaredomains, path=server_path)
    analysis(query={}, projection={'_id': 0})
    # returns a pandas data frame
    data_frame = analysis.time_series_analysis_per_month('mongoDate')
    # store analysis results
    analysis.data_frame_to_csv(data_frame, "perdayTimeSeriesMalware")
    analysis.data_frame_to_json(data_frame, "perdayTimeSeriesMalware")
    top5 = analysis.top_n(5,"Subcategory", "malware-top-subcategories")
    print(top5)

    ''' Export current MongoDB collection instance '''
    # malwaredomains_collection = ExportCollectionData(collection=db.threats.malwaredomains, path=server_path)
    # malwaredomains_collection(query={}, projection={"_id": 0, "mongoDate": 0, "mongoDate-CTI": 0})
    # malwaredomains_collection.export_collection_to_json("dataset-malware")
    #
    # csv_header = ["Category", "Entity-Type", "Domain", "Subcategory", "Submitted-By", "TimestampUTC", "DatetimeUTC", "TimestampUTC-CTI", "DatetimeUTC-CTI", "State"]
    # malwaredomains_collection.export_collection_to_csv("dataset-malware", csv_header)
