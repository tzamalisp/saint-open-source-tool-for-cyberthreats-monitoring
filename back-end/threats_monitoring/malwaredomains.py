from downloader import Downloader
from descriptive_analysis import *
from export_collection_data import *
from pymongo import MongoClient
from pymongo.errors import PyMongoError
from bs4 import BeautifulSoup
from datetime import datetime
import ssl
import re

ssl._create_default_https_context = ssl._create_unverified_context


def connect_to_mongodb():
    """ This function implements the connection to mongoDB
        @returns
            connection  (MongoClient or None)   a MongoClient object to handle the connection. None on failure
    """

    # connect to database
    try:
        connection = MongoClient('XXX.XXX.XXX.XXX', 27017)
        db = connection.admin
        db.authenticate('xxxxxx', 'xxxXXXxxxXX')
        return db

    except PyMongoError as e:
        print("Connection to Data Base failed: ", e)
        return None

# -------------------------------------------------------------------------------------------------------------------- #


def crawl_malware_domains(url):
    """ This function crawls the malware domain indicator and returns all the dataset links to be downloaded and scraped
        later.
    @param
        url     (string)        url of the indicator web page
    @return
    """

    print('Crawling site: ', url)
    downloader = Downloader()
    print(url)
    html = downloader(url)

    soup = BeautifulSoup(html, 'html5lib')
    possible_links = soup.find_all('a')

    htmlLinks, htmlRemovedLinks = list([]), list([])

    for link in possible_links :
        if link.has_attr('href') and link.attrs['href'][0].isdigit():
            # construct full path using function parameter url = 'https://mirror.uce.edu.ec/malwaredomains/'
            full_link = '{}{}'.format(url, link.attrs['href'])
            htmlLinks.append(full_link)
        elif link.has_attr('href') and link.attrs['href'].startswith('removed-domains-'):
            # in this loop we gather all the removed ip lists
            full_link = '{}{}'.format(url, link.attrs['href'])
            htmlRemovedLinks.append(full_link)

    return {"blocked": htmlLinks, "removed":  htmlRemovedLinks}

# -------------------------------------------------------------------------------------------------------------------- #


def scrape_and_model_content(html_links):

    modeled_data = list([])
    downloader = Downloader()
    exp = re.compile("""[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]""")

    for link in html_links["blocked"]:
        content = downloader(link)

        for row in content.split('\n'):

            data = [word for word in row.strip('\t').split('\t')]

            if len(data) != 4:
                continue

            date_regex = exp.match(data[3])

            if date_regex is not None:
                # prepare time values from indicators
                data[3] = date_regex.group()
                datetimeObject = datetime.strptime(data[3], '%Y%m%d')
                datetime_utc_string = str(datetimeObject.strftime('%Y-%m-%d'))
                datetime_utc = datetime.strptime(datetime_utc_string,'%Y-%m-%d')
                timestamp_utc = float(datetime_utc.timestamp())
            else:
                continue

            # prepare CTI time
            datetime_utc_cti_string = str(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'))
            datetime_utc_cti = datetime.strptime(datetime_utc_cti_string, '%Y-%m-%d %H:%M:%S')
            timestamp_utc_cti = float(datetime_utc_cti.timestamp())

            data_dict = {
                "Category": "Malware",
                "Entity-Type": "Domain",
                "Domain": data[0],
                "Subcategory": data[1],
                "Submitted-By": data[2],
                "TimestampUTC": timestamp_utc,
                "mongoDate": datetime_utc,
                "DatetimeUTC": datetime_utc_string,
                "TimestampUTC-CTI": timestamp_utc_cti,
                "mongoDate-CTI": datetime_utc_cti,
                "DatetimeUTC-CTI": datetime_utc_cti_string,
                "State": "Blocked"
            }

            modeled_data.append(data_dict)

    for link in html_links["removed"]:
           content = downloader(link)

           for row in content.split('\n'):
                data = [word for word in row.strip('\r').split('\t') if word != '#' and word != '']

                if len(data) != 4:
                    continue

                date_regex = exp.match(data[3])
                if date_regex is not None:
                    # prepare time values from indicators
                    data[3] = date_regex.group()
                    datetimeObject = datetime.strptime(data[3], '%Y%m%d')
                    datetime_utc_string = str(datetimeObject.strftime('%Y-%m-%d'))
                    datetime_utc = datetime.strptime(datetime_utc_string,'%Y-%m-%d')
                    timestamp_utc = float(datetime_utc.timestamp())
                else:
                    continue

                # prepare CTI time
                datetime_utc_cti_string = str(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'))
                datetime_utc_cti = datetime.strptime(datetime_utc_cti_string, '%Y-%m-%d %H:%M:%S')
                timestamp_utc_cti = float(datetime_utc_cti.timestamp())

                data_dict = {
                    "Category": "Malware",
                    "Entity-Type": "Domain",
                    "Domain": data[0],
                    "Subcategory": data[1],
                    "Submitted-By": data[2],
                    "TimestampUTC": timestamp_utc,
                    "mongoDate": datetime_utc,
                    "DatetimeUTC": datetime_utc_string,
                    "TimestampUTC-CTI": timestamp_utc_cti,
                    "mongoDate-CTI": datetime_utc_cti,
                    "DatetimeUTC-CTI": datetime_utc_cti_string,
                    "State": "Removed"
                }

                modeled_data.append(data_dict)

    return  modeled_data

# -------------------------------------------------------------------------------------------------------------------- #


if __name__ == "__main__":

    crawling_time = datetime.utcnow()
    print("Report on", crawling_time.strftime('%Y-%m-%d %H:%M:%S'), '\n')

    ''' Scrape Indicator '''
    # URL of the indicator
    URL = 'https://mirror.uce.edu.ec/malwaredomains/'
    # crawl indicator
    html_links = crawl_malware_domains(URL)
    # scrape data from links
    data = scrape_and_model_content(html_links)

    ''' Store instance of scraped data in MongoDB '''
    print("\nConnecting to MongoDB Data Base")
    db = connect_to_mongodb()
    db.threats.malwaredomains.drop()

    print("Storing Data...", end=' ')
    res = db.threats.malwaredomains.insert_many(data)

    print("Completed")
    print("Documents Inserted: ", len(res.inserted_ids))
    # ---------------------------------------------------------------------------------------------------------------- #

    """ Descriptive Analysis """
    # set path of produced files
    path = '/var/www/html/saint/indicators2018/malware/'

    # Time Series Section
    from pathlib import Path
    time_series_file = Path(path + 'permonthTimeSeriesMalware.json')

    # set the appropriate query
    if time_series_file.exists():
        # retrieve ONLY current month's data
        month_start, month_end = datetime_limits_of_month(crawling_time)
        query = {"mongoDate": {"$gte": month_start, "$lte": month_end}}
        print("\nFile permonthTimeSeriesMalware.json exists. Retrieve current month's attacks")
    else:
        # there is no analysis file, so retrieve the entire collection and create one
        query = {}
        print('\nFile permonthTimeSeriesMalware.json does not exist. Retrieve the entire collection')
    # set the projection
    projection = {"_id": 0, "mongoDate": 1}

    try:
        results_cursor = db.threats.malwaredomains.find(query, projection)
        attack_data_frames = time_series_analysis_per_month(results_cursor, mongo_date_type='mongoDate')
        # updates both csv and json files. Specify name with NO EXTENSION
        update_time_series_analysis_files(attack_data_frames, "permonthTimeSeriesMalware", path)
    except Exception as e:
        print("__main__ > Descriptive Analysis Phase > Time Series Section: ", e)
        # destroy cursor, it won't be used again
        results_cursor = None

    # Barplot Section / Top 5 Subcategories
    query = {}
    projection = {"_id": 0}

    try:
        results_cursor = db.threats.malwaredomains.find(query, projection)
        # find last weeks top 10 countries and save them to csv and json. Specify name with NO EXTENSION
        top_subcategories = top_n(results_cursor, 5, 'Subcategory', 'malware-top-subcategories', path)
        print("\nLast Week's Top Botnet Countries\n", top_subcategories)
    except Exception as e:
        print("__main__ > descriptive analysis phase > Barplot Analysis: ", e)
        # destroy cursor, it won't be used again
        results_cursor = None
    # ---------------------------------------------------------------------------------------------------------------- #

    """ Export Datasets """
    # last datetime moment of this month must be based on crawling time to ensure synchronization
    start_of_month, end_of_month = datetime_limits_of_month(utcnow=crawling_time)

    # If you want to produce a dataset of a past month just uncomment below and modify the example
    # and change dataset name too!
    # start_of_month, end_of_month = datetime_limits_of_month(utcnow=None, set_year=2018, set_month=8)

    export_path = path + 'dataset-malware/'
    malwaredomains_csv_header = ["Category", "Entity-Type", "Domain", "Subcategory", "Submitted-By", "TimestampUTC",
                          "DatetimeUTC", "TimestampUTC-CTI", "DatetimeUTC-CTI", "State"]
    dataset_name = 'malware_{0}_{1}.csv'.format(crawling_time.year, crawling_time.month)

    print("\nRetrieving MongoDB's Collection...")
    try:
        query = {"mongoDate": {"$gte": start_of_month, "$lte": end_of_month}}
        projection = {"_id": 0, "mongoDate": 0, "mongoDate-CTI": 0}
        results_cursor = db.threats.malwaredomains.find(query, projection)
        if results_cursor.count() == 0:
           print("No data entries returned for this month {0}. "
                 "dataset-malware won't be created".format(crawling_time.strftime('%Y-%m-%d')))
        else:
            export_to_csv(results_cursor, dataset_name, malwaredomains_csv_header, export_path)
            zip_directory(export_path, "dataset-malware.zip", path)
    except Exception as e:
        print("__main__ > Export Datasets Phase: ", e)
        # destroy cursor, it won't be used again
        results_cursor = None

