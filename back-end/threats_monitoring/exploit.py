from lxml.html import fromstring
from pymongo import MongoClient
from downloader import Downloader
from descriptive_analysis import *
from export_collection_data import *
from bs4 import BeautifulSoup
import re


def connect_to_mongodb():
    """ This function implements the connection to the mongoDb
        @returns
            connection  (MongoClient)   a MongoClient object to handle the connection
    """
    # connect to database
    connection = MongoClient('XXX.XXX.XXX.XXX', 27017)
    db = connection.admin
    db.authenticate('xxxxxx', 'xxxXXXxxxXX')

    return db

# -------------------------------------------------------------------------------------------------------------------- #


def link_crawler(start_url, link_regex, delay=1, max_depth=5):
    """ Crawl from the given start URL following links matched by link_regex.
        @parameters
            start_url   (str)   web site to start crawl
            link_regex  (str)   regex to match for links
            delay       (int)   seconds to throttle between requests to one domain (default: 3)
            max_depth   (int)   maximum crawl depth (to avoid traps) (default: 4)
        @returns
            res         (dict)  the crawling result in form as below
                                {"discovered_paginations": seen, "discovered_exploits": exploits_set, "data": data_list}
    """

    crawl_queue = [start_url]       # crawl_queue is meant to contain all the pagination links
    seen = {start_url: 0}           # seen is a dictionary. Keys are pagination urls and values are crawl depth
    # note: crawl depth is the number of discoveries for a specific link
    exploits_set = set()            # this set contains all threat links. Ensures that each one is examined only once
    data_list = []                  # each item is a dictionary modeled for mongoDB

    downloader = Downloader(delay=delay, cache={})
    # practically while(1)
    while crawl_queue:
        url = crawl_queue.pop()
        # get crawl depth of this url
        depth = seen.get(url, 0)
        if depth == max_depth:
            print('\t\tSkipping %s due to depth' % url)
            continue

        html = downloader(url)
        # something went wrong skip this page
        if html is None:
            print('link_crawler > Download Error: No content was returned by' + url)
            continue

        ''' Crawl this page to discover links '''
        tree = fromstring(html)                               # forming html tree
        all_html_links = tree.iterlinks()                     # get all links from html. returns a generator
        all_html_links = list([x for x in all_html_links])    # convert generator to list in order to access content

        ''' This for loop will filter and process the links of interest'''
        for i in range(len(all_html_links)):
            # for all discovered url we are interested in those
            # that contain /ghdb/ parameter in their path. These are the exploit urls to scrape
            ''' Filter Threat Links '''
            if re.search('/ghdb/', all_html_links[i][2]):
                # (< Element a at 0x2223393bf98 >, 'href', 'https://www.exploit-db.com/ghdb/4671/', 0)
                # the above tuple corresponds to the form of an all_html_links item
                exploit_url = all_html_links[i][2]    # selects the url from every tuple in the list

                if exploit_url not in exploits_set:
                    # add exploit url in the set. Thus it will be examined only once
                    exploits_set.add(exploit_url)

                    ''' Scrape a web page! '''
                    # first download the page to scrape
                    exploit_html = downloader(exploit_url)
                    # get a list of values
                    values = scrape_it(exploit_html)
                    # process the values, clean them and model them. Returned as dictionary
                    data = validate_values(values)
                    # store modeled data in a list
                    data_list.append(data)

            ''' Filter Pagination Links '''
            # after studying pagination links we found out they always contain '/?pg=' substring
            # also avoid relative links
            if 'https://' in all_html_links[i][2] and re.search(link_regex, all_html_links[i][2]):

                pagination_link = all_html_links[i][2]
                # These are the useful links
                # If not seen again process them
                if pagination_link not in seen:
                    # print('\tDiscovered for first time...')
                    seen[pagination_link] = depth + 1
                    print("\tDiscovered Link: {0} \tDepth: {1}".format(pagination_link, seen[pagination_link]))
                    crawl_queue.append(pagination_link)
                else:
                    seen[pagination_link]: depth + 1

    # create a dictionary with all the results
    res = {"discovered_paginations": seen, "discovered_exploits": exploits_set, "data": data_list}

    return res

# -------------------------------------------------------------------------------------------------------------------- #


# CURRENTLY NOT IN USE
def get_links(html):
    """ Finds all the regular expressions within a webpage
        @parameter
            html    (str)
        @returns
            no name (list)  a list of all discovered links
    """

    webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE)

    return webpage_regex.findall(html)

# -------------------------------------------------------------------------------------------------------------------- #


def scrape_it(html):
    """ Scrapes all the need data from the downloaded web page
        @parameter
            html    (str)   html source code (never None) of downloaded page
        @return
            values  (list)  list with all scraped values
    """

    soup = BeautifulSoup(html, 'lxml')
    tree = fromstring(html)

    try:
        # scrape url
        google_search = soup.find('a', target="_blank", class_="external")
        google_search = google_search['href']
        # google_search = tree.cssselect('a.external, a[target=_blank]')[0].text_content()
        # google_search = tree.xpath('/html/body/div/div/div/main/section/div/table/tbody/tr/td/a[@class="external"]/
        # @href')[0]
        # print(google_search)
        # google dork description
    except Exception as er:
        print("link_crawler > scrape_it error for google_search: ", er)
        google_search = None

    try:
        dork_description = tree.cssselect('table.category-list > tbody > tr > td')[0].text_content()
        dork_description = [word for word in dork_description.split() if word not in "Google Dork Description"]
        google_dork_description = ""
        for word in dork_description:
            google_dork_description += word + " "
    except Exception as error:
        print("link_crawler > scrape_it error for google_dork_description: ", error)
        google_dork_description = None

    try:
        # enb-id
        enb_id = tree.cssselect('table.category-list > tbody > tr > td')[3].text_content().split()[1]
    except Exception as exc:
        print("link_crawler > scrape_it error for enb_id: ", exc)
        enb_id = None

    try:
        # scrape date
        utc_date = tree.cssselect('time')[0].text_content()
    except Exception as err:
        print("link_crawler > scrape_it error for utc_date: ", err)
        utc_date = None

    try:
        # scrape GHDB-ID
        ghdb_content = tree.cssselect('table.category-list > tbody > tr > td')[1].text_content()
        ghdb_id = [int(number) for number in ghdb_content.split() if number.isdigit()]
        ghdb_id = ghdb_id[0]
    except Exception as exce:
        print("link_crawler > scrape_it error for ghdb_id: ", exce)
        ghdb_id = None

    try:
        # scrape description
        des = tree.cssselect('div#container > pre')[0].text_content().split("\n")
        des = [word for word in des if not word.startswith("\r")]
        des = [word.strip("\r") for word in des]
        description = ""
        for word in des[0:]:
            description += word + " "
    except Exception as excep:
        print("link_crawler > scrape_it error for description ", excep)
        description = None

    try:
        # scrape author
        author_content = tree.cssselect('table.category-list > tbody > tr > td')[5].text_content()
        author_content = [word for word in author_content.split() if word != "Author:"]
        author = ""
        for name in author_content[0:]:
            author += name + " "
    except Exception as ex:
        print("link_crawler > scrape_it error for enb_id: ", ex)
        author = None

    return [google_search, google_dork_description, utc_date, ghdb_id, author, enb_id, description]

# -------------------------------------------------------------------------------------------------------------------- #


def validate_values(values):
    """ The function validates the data type of each element and then models data in a dict json like format
        :parameter
            values  (list)  list of values to be cleaned
        :return
            data    (dict)  a dictionary that contains clean values to be inserted to DB
    """

    if values[3] is not None:
        _id = int(values[3])
    else:
        _id = None

    if values[0] is not None:
        google_search = str(values[0].strip())
    else:
        google_search = None

    if values[1] is not None:
        google_dork_description = str(values[1])
    else:
        google_dork_description = None

    if values[6] is not None:
        description = str(values[6].strip())
    else:
        description = None

    if values[5] is not None:
        enb_id = str(values[5].strip())
    else:
        enb_id = None

    if values[2] is not None:
        datetime_utc_string = str(values[2].strip())
        datetime_utc = datetime.strptime(datetime_utc_string, '%Y-%m-%d')
        timestamp_utc = float(datetime_utc.timestamp())
    else:
        datetime_utc_string = None
        datetime_utc = None
        timestamp_utc = None

    if values[4] is not None:
        author = str(values[4].strip())
    else:
        author = None

    datetime_utc_cti_string = str(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'))
    datetime_utc_cti = datetime.strptime(datetime_utc_cti_string, '%Y-%m-%d %H:%M:%S')
    timestamp_utc_cti = float(datetime_utc_cti.timestamp())

    data = {
        "_id": _id,
        "Category": "DDoS",
        "Subcategory": "Pages Containing Login Portal",
        "Entity-Type": "URI",
        "Google-Search": google_search,
        "Google-Dork-Description": google_dork_description,
        "Description": description,
        "ENB-ID": enb_id,
        "TimestampUTC": timestamp_utc,
        "mongoDate": datetime_utc,
        "DatetimeUTC": datetime_utc_string,
        "TimestampUTC-CTI": timestamp_utc_cti,
        "mongoDate-CTI": datetime_utc_cti,
        "DatetimeUTC-CTI": datetime_utc_cti_string,
        "Author": author
    }

    return data

    # ---------------------------------------------------------------------------------------------------------------- #


if __name__ == "__main__":

    crawling_time = datetime.utcnow()
    print("Report on", crawling_time.strftime('%Y-%m-%d %H:%M:%S'), '\n')

    URL = 'https://www.exploit-db.com/google-hacking-database/12/?pg=1'

    # crawl website respecting the server
    # '/?pg=' discovers all the pagination links within a web page
    # results = {"discovered_paginations": seen, "discovered_exploits": exploits_set, "data": data_list}
    results = link_crawler(URL, '/?pg=', delay=1)
    print('Scraping finished')

    docs = results["data"]
    if len(docs) == 0:
        print("No data were scraped\nExiting Program")
        exit(0)

    ''' Store data in MongoDB '''
    print("\nConnecting to MongoDB Data Base")
    db = connect_to_mongodb()
    docs_match_update, docs_modified_update, docs_inserted = 0, 0, 0

    print("Storing Data...", end=' ')
    # store most recent today's data
    for doc in docs:
        upd_res = db.threats.exploitDataBase.update_one({"_id": doc["_id"]}, {"$set": doc}, upsert=True)
        docs_match_update += upd_res.matched_count
        docs_modified_update += upd_res.modified_count
        if upd_res.upserted_id is not None:
            docs_inserted += 1

    print("Completed")
    print("Documents Inserted: ", docs_inserted)
    print("Documents Matched Update Filter: ", docs_match_update)
    print("Documents Modified: ", docs_modified_update)
    # ---------------------------------------------------------------------------------------------------------------- #

    """ Descriptive Analysis """
    # set path of produced files
    path = '/var/www/html/saint/indicators2018/ddos/'

    # Time Series Section
    from pathlib import Path
    time_series_file = Path(path + 'perdayTimeSeriesDDoS.json')

    # set the appropriate query
    if time_series_file.exists():
        # retrieve ONLY current day's data
        today_start, today_end = today_datetime(crawling_time)
        query = {"mongoDate": {"$gte": today_start, "$lte": today_end}}
        print("\nFile perdayTimeSeriesDDoS.json exists. Retrieve current days's attacks")
    else:
        # there is no analysis file, so retrieve the entire collection and create one
        query = {}
        print('\nFile perdayTimeSeriesDDoS.json does not exist. Retrieve the entire collection')
    # set the projection
    projection = {"_id": 0, "mongoDate": 1}

    try:
        results_cursor = db.threats.exploitDataBase.find(query, projection)
        attack_data_frames = time_series_analysis(results_cursor, mongo_date_type='mongoDate')
        # updates both csv and json files. Specify name with NO EXTENSION
        update_time_series_analysis_files(attack_data_frames, "perdayTimeSeriesDDoS", path)
    except Exception as e:
        print("__main__ > Descriptive Analysis Phase > Time Series Section: ", e)
        # destroy cursor, it won't be used again
        results_cursor = None
    # ---------------------------------------------------------------------------------------------------------------- #

    """ Export Datasets """


    start_of_month, end_of_month = datetime_limits_of_month(utcnow=crawling_time)
    ddos_csv_header = ["Category", "Subcategory", "Entity-Type", "Google-Search", "Google-Dork-Description",
                       "Description", "ENB-ID", "TimestampUTC", "DatetimeUTC", "TimestampUTC-CTI",
                       "DatetimeUTC-CTI", "Author"]
    dataset_name = 'ddos_{0}_{1}.csv'.format(crawling_time.year, crawling_time.month)

    export_path = path + 'dataset-ddos/'

    # export the entire collection as it contains only few entries
    print("\nRetrieving MongoDB's Collection...")
    try:
        query = {"mongoDate": {"$gte": start_of_month, "$lte": end_of_month}}
        projection = {"_id": 0, "mongoDate": 0, "mongoDate-CTI": 0}
        results_cursor = db.threats.exploitDataBase.find(query, projection)
        if results_cursor.count() == 0:
           print("No data entries returned for this month {0}. "
                 "dataset-ddos won't be created".format(crawling_time.strftime('%Y-%m-%d')))
        else:
            export_to_csv(results_cursor, dataset_name, ddos_csv_header, export_path)
            zip_directory(export_path, "dataset-ddos.zip", path)
    except Exception as e:
        print("__main__ > Export Datasets Phase: ", e)
        # destroy cursor, it won't be used again
        results_cursor = None

